{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEPEz4tvvkVe"
      },
      "source": [
        "DMML Assignment 1\n",
        "\n",
        "Submitted by : Trishita and Sowmya"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This file contains two implementations of the finding frequent itemsets that we tested for various cases. While both produced correct results, we observed that Code 2 performed faster than Code 1. Therefore, we chose Code 2 for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNHxMA7ZSGhe",
        "outputId": "27e989d9-8992-4721-c316-48d25b9573a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Code 1:**\n",
        "- **Purpose**: Finds frequent $ K $-itemsets from a dataset using a recursive approach to generate all possible $ K $-itemsets.\n",
        "- **Helper Functions**:\n",
        "  1. **`load_vocab(vocab_file)`**:\n",
        "     - Input: Vocabulary file path.\n",
        "     - Output: Dictionary mapping word IDs to words.\n",
        "  2. **`load_docword(docword_file)`**:\n",
        "     - Input: Document-word file path.\n",
        "     - Output: Two dictionaries:\n",
        "       - `doc_words`: Maps document IDs to sets of word IDs.\n",
        "       - `word_counts`: Maps word IDs to their total counts across all documents.\n",
        "  3. **`generate_k_itemsets(words, K, index, current_itemset)`**:\n",
        "     - Input: List of words, target size $ K $, current index, and current itemset being built.\n",
        "     - Output: List of all $ K $-itemsets generated recursively.\n",
        "  4. **`find_frequent_itemsets(vocab_file, docword_file, K, F)`**:\n",
        "     - Input: Vocabulary file, document-word file, $ K $ (itemset size), $ F $ (frequency threshold).\n",
        "     - Output: Execution time and count of frequent $ K $-itemsets.\n",
        "     - Workflow:\n",
        "       - Filters frequent words based on $ F $.\n",
        "       - Generates all $ K $-itemsets for each document using recursion.\n",
        "       - Counts occurrences of each $ K $-itemset and filters by $ F $."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kYVoe4Q3grgi"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Load vocabulary (wordID â†’ word mapping)\n",
        "def load_vocab(vocab_file):\n",
        "    vocab_dict = {}\n",
        "    with open(vocab_file, \"r\") as file:\n",
        "        for i, word in enumerate(file, start=1):\n",
        "            vocab_dict[i] = word.strip()\n",
        "    return vocab_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vz9aH6isgwL9"
      },
      "outputs": [],
      "source": [
        "# Load document-word data\n",
        "def load_docword(docword_file):\n",
        "    doc_words = {}  # Dictionary {docID: set(words)}\n",
        "    word_counts = {}  # Dictionary {wordID: count}\n",
        "\n",
        "    with open(docword_file, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    D, W, NNZ = map(int, lines[:3])  # Read metadata\n",
        "\n",
        "    for line in lines[3:]:\n",
        "        docID, wordID, count = map(int, line.split())\n",
        "\n",
        "        if docID not in doc_words:\n",
        "            doc_words[docID] = set()\n",
        "        doc_words[docID].add(wordID)\n",
        "\n",
        "        word_counts[wordID] = word_counts.get(wordID, 0) + count\n",
        "\n",
        "    return doc_words, word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3Vu2NM1Jg0PF"
      },
      "outputs": [],
      "source": [
        "# Recursive function to generate K-itemsets\n",
        "def generate_k_itemsets(words, K, index=0, current_itemset=[]):\n",
        "    if len(current_itemset) == K:\n",
        "        return [tuple(current_itemset)]  # Base case: return the itemset\n",
        "\n",
        "    if index >= len(words):\n",
        "        return []  # Stop if out of words\n",
        "\n",
        "    # Recursive case: pick the current word and move forward OR skip it\n",
        "    return generate_k_itemsets(words, K, index + 1, current_itemset + [words[index]]) + generate_k_itemsets(words, K, index + 1, current_itemset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SQXzBEzPG6XQ"
      },
      "outputs": [],
      "source": [
        "def find_frequent_itemsets(vocab_file, docword_file, K, F): # Main function to find frequent itemsets\n",
        "    print(f\"\\nFinding Frequent {K}-Itemsets with F={F}...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load data\n",
        "    vocab_dict = load_vocab(vocab_file)\n",
        "    doc_words, word_counts = load_docword(docword_file)\n",
        "\n",
        "    # Step 1: Filter words that appear at least F times\n",
        "    frequent_words = {wordID for wordID, count in word_counts.items() if count >= F}\n",
        "\n",
        "    # Step 2: Process each document and generate K-itemsets\n",
        "    itemset_counts = {}\n",
        "\n",
        "    for words in doc_words.values():   # words - set\n",
        "        valid_words = sorted(wordID for wordID in words if wordID in frequent_words)\n",
        "\n",
        "        if len(valid_words) >= K:\n",
        "            k_itemsets = generate_k_itemsets(valid_words, K)\n",
        "\n",
        "            for itemset in k_itemsets:\n",
        "                itemset_counts[itemset] = itemset_counts.get(itemset, 0) + 1\n",
        "\n",
        "    # Step 3: Filter itemsets by frequency threshold\n",
        "    frequent_itemsets = {\n",
        "        tuple(vocab_dict[wordID] for wordID in itemset): count\n",
        "        for itemset, count in itemset_counts.items()\n",
        "        if count >= F\n",
        "    }\n",
        "\n",
        "    execution_time = time.time() - start_time\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "    print(f\"Total Frequent {K}-Itemsets Found: {len(frequent_itemsets)}\")\n",
        "\n",
        "    if frequent_itemsets:\n",
        "        print(\"\\nFirst 5 Frequent K-itemsets:\")\n",
        "        for itemset, count in list(frequent_itemsets.items())[:5]:  # Show first 10 examples\n",
        "            print(f\"Itemset: {itemset}, Count: {count}\")\n",
        "    else:\n",
        "        print(\"No frequent itemsets found.\")\n",
        "\n",
        "    return execution_time, len(frequent_itemsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb106DOIHvdy"
      },
      "source": [
        "Test runs with KOS Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C_VYJDtiHupT"
      },
      "outputs": [],
      "source": [
        "vocab_file = \"/content/drive/My Drive/DMML_Assignment_1/vocab.kos.txt\"\n",
        "docword_file = \"/content/drive/My Drive/DMML_Assignment_1/docword.kos.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rVQBmvucOID"
      },
      "source": [
        "Example 1 : K =2, F= 700"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4M70plGHAyY",
        "outputId": "5593c543-75dc-48c8-c3e9-531434a6b14b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finding Frequent 2-Itemsets with F=700...\n",
            "Execution Time: 1.24 seconds\n",
            "Total Frequent 2-Itemsets Found: 23\n",
            "\n",
            "First 5 Frequent K-itemsets:\n",
            "Itemset: ('administration', 'bush'), Count: 748\n",
            "Itemset: ('bush', 'general'), Count: 1250\n",
            "Itemset: ('bush', 'house'), Count: 873\n",
            "Itemset: ('bush', 'kerry'), Count: 1195\n",
            "Itemset: ('bush', 'president'), Count: 834\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1.2379562854766846, 23)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "K = 2  # Set desired K-itemset size\n",
        "F = 700  # Set minimum frequency threshold\n",
        "\n",
        "find_frequent_itemsets(vocab_file, docword_file, K, F)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9CNPP3McUYY"
      },
      "source": [
        "Example 2 : K =3, F= 700"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMq5URwpIWVG",
        "outputId": "abd0ff52-8376-411b-e9ff-4bc887dfb343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finding Frequent 3-Itemsets with F=700...\n",
            "Execution Time: 3.29 seconds\n",
            "Total Frequent 3-Itemsets Found: 1\n",
            "\n",
            "First 5 Frequent K-itemsets:\n",
            "Itemset: ('bush', 'general', 'kerry'), Count: 926\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(3.2868590354919434, 1)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_frequent_itemsets(vocab_file, docword_file, 3, F = 700)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnJufij9cW4O"
      },
      "source": [
        "Example 3 : K =5, F= 700"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIcSETgLV4YD",
        "outputId": "14eb6221-91a5-4233-fd53-b00c159ccf5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finding Frequent 5-Itemsets with F=700...\n",
            "Execution Time: 103.57 seconds\n",
            "Total Frequent 5-Itemsets Found: 0\n",
            "No frequent itemsets found.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(103.57049608230591, 0)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_frequent_itemsets(vocab_file, docword_file, K = 5, F = 700)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Code 2:**\n",
        "- **Purpose**: Implements the Apriori algorithm to efficiently find frequent $ K $-itemsets using an iterative candidate generation and pruning strategy.\n",
        "- **Helper Functions**:\n",
        "  1. **`read_vocab(vocab_file)`**:\n",
        "     - Input: Vocabulary file path.\n",
        "     - Output: Dictionary mapping word IDs to words.\n",
        "  2. **`read_docword(docword_file)`**:\n",
        "     - Input: Document-word file path.\n",
        "     - Output: Dictionary mapping word IDs to sets of documents containing them.\n",
        "  3. **`apriori(word_docs, K, F)`**:\n",
        "     - Input: Word-document dictionary, $ K $ (itemset size), $ F $ (frequency threshold).\n",
        "     - Output: Sorted list of frequent $ K $-itemsets with their counts.\n",
        "     - Workflow:\n",
        "       - Starts with frequent 1-itemsets.\n",
        "       - Iteratively generates $ k $-itemsets from $ (k-1) $-itemsets using efficient pruning.\n",
        "       - Counts support for candidates and filters infrequent ones.\n",
        "  4. **`main(vocab_file, docword_file, K, F)`**:\n",
        "     - Input: Vocabulary file, document-word file, $ K $, $ F $.\n",
        "     - Output: Prints execution time, total frequent itemsets, and examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-iNJmKnCwrRi"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "def read_vocab(file_path):\n",
        "    \"\"\" Read the vocabulary file and map word IDs to words. \"\"\"\n",
        "    vocab = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for i, line in enumerate(f, start=1):\n",
        "            vocab[i] = line.strip()\n",
        "    return vocab\n",
        "\n",
        "def read_docword(file_path):\n",
        "    \"\"\" Read the document-word file and store word occurrences efficiently. \"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        D = int(f.readline().strip())  # Number of documents\n",
        "        W = int(f.readline().strip())  # Number of words\n",
        "        NNZ = int(f.readline().strip())  # Nonzero entries\n",
        "\n",
        "        word_docs = defaultdict(set)  # wordID -> set of documents containing it\n",
        "        for _ in range(NNZ):\n",
        "            doc_id, word_id, count = map(int, f.readline().strip().split())\n",
        "            word_docs[word_id].add(doc_id)\n",
        "\n",
        "    return word_docs\n",
        "\n",
        "def apriori(word_docs, K, F):\n",
        "    \"\"\" Optimized Apriori algorithm for large datasets. \"\"\"\n",
        "\n",
        "    # Step 1: Find frequent 1-itemsets\n",
        "    freq_itemsets = { (word,): docs for word, docs in word_docs.items() if len(docs) >= F }\n",
        "\n",
        "    # Step 2: Generate k-itemsets iteratively\n",
        "    for k in range(2, K + 1):\n",
        "        candidates = set()\n",
        "        freq_keys = list(freq_itemsets.keys())  # List of current frequent itemsets\n",
        "\n",
        "        # Generate candidate itemsets of size k using (k-1)-itemsets\n",
        "        for i in range(len(freq_keys)):\n",
        "            for j in range(i + 1, len(freq_keys)):\n",
        "                a, b = freq_keys[i], freq_keys[j]\n",
        "\n",
        "                # Merge only if first (k-2) elements are same (Efficient pruning)\n",
        "                if a[:-1] == b[:-1]:\n",
        "                    new_itemset = tuple(sorted(set(a) | set(b)))  # Union\n",
        "                    if len(new_itemset) == k:\n",
        "                        candidates.add(new_itemset)\n",
        "\n",
        "        # Count support for candidate itemsets\n",
        "        new_freq_itemsets = {}\n",
        "        for c in candidates:\n",
        "            intersect_docs = set.intersection(*(word_docs[word] for word in c))\n",
        "            if len(intersect_docs) >= F:\n",
        "                new_freq_itemsets[c] = intersect_docs\n",
        "\n",
        "        # If no new frequent itemsets, break early\n",
        "        if not new_freq_itemsets:\n",
        "            return []\n",
        "\n",
        "        freq_itemsets = new_freq_itemsets\n",
        "\n",
        "    # Convert sets to counts for readability\n",
        "    return sorted([(itemset, len(docs)) for itemset, docs in freq_itemsets.items()],\n",
        "                  key=lambda x: x[1], reverse=True)\n",
        "\n",
        "def main(vocab_file, docword_file, K, F):\n",
        "    print(\"Reading dataset...\")\n",
        "    word_docs = read_docword(docword_file)\n",
        "    vocab = read_vocab(vocab_file)\n",
        "\n",
        "    print(f\"Running Apriori for K={K}, F={F}\")\n",
        "    start_time = time.time()\n",
        "    frequent_itemsets = apriori(word_docs, K, F)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nTime taken: {elapsed:.2f} seconds\")\n",
        "\n",
        "    if not frequent_itemsets:\n",
        "        print(\"\\nNo itemsets found.\")\n",
        "    else:\n",
        "        print(f\"\\nTotal Frequent K-itemsets Found: {len(frequent_itemsets)}\")\n",
        "        print(\"Frequent K-itemsets:\")\n",
        "        for itemset, count in frequent_itemsets:\n",
        "            print(f\"Itemset: {tuple(vocab[word] for word in itemset)}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVq2JwLerVHz"
      },
      "source": [
        "Example 1 : K =2, F= 700"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOziMbZprV-h",
        "outputId": "176d0233-5499-4fd5-90c0-90af55948e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading dataset...\n",
            "Running Apriori for K=2, F=700\n",
            "\n",
            "Time taken: 0.05 seconds\n",
            "\n",
            "Total Frequent K-itemsets Found: 23\n",
            "Frequent K-itemsets:\n",
            "Itemset: ('bush', 'general'), Count: 1250\n",
            "Itemset: ('bush', 'kerry'), Count: 1195\n",
            "Itemset: ('general', 'kerry'), Count: 1064\n",
            "Itemset: ('bush', 'war'), Count: 994\n",
            "Itemset: ('bush', 'democratic'), Count: 937\n",
            "Itemset: ('democratic', 'kerry'), Count: 922\n",
            "Itemset: ('democratic', 'primary'), Count: 894\n",
            "Itemset: ('bush', 'poll'), Count: 876\n",
            "Itemset: ('bush', 'house'), Count: 873\n",
            "Itemset: ('kerry', 'poll'), Count: 845\n",
            "Itemset: ('bush', 'president'), Count: 834\n",
            "Itemset: ('bush', 'republicans'), Count: 794\n",
            "Itemset: ('bush', 'democrats'), Count: 783\n",
            "Itemset: ('bush', 'election'), Count: 766\n",
            "Itemset: ('democratic', 'general'), Count: 763\n",
            "Itemset: ('general', 'poll'), Count: 756\n",
            "Itemset: ('democratic', 'poll'), Count: 755\n",
            "Itemset: ('administration', 'bush'), Count: 748\n",
            "Itemset: ('general', 'war'), Count: 745\n",
            "Itemset: ('democratic', 'democrats'), Count: 740\n",
            "Itemset: ('bush', 'time'), Count: 726\n",
            "Itemset: ('bush', 'media'), Count: 721\n",
            "Itemset: ('election', 'general'), Count: 715\n"
          ]
        }
      ],
      "source": [
        "main(vocab_file, docword_file, K = 2, F = 700)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN9t7fjArcd-",
        "outputId": "8ad122a0-7bac-4b9a-dfde-489d6065f8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading dataset...\n",
            "Running Apriori for K=5, F=700\n",
            "\n",
            "Time taken: 0.06 seconds\n",
            "\n",
            "No itemsets found.\n"
          ]
        }
      ],
      "source": [
        "main(vocab_file, docword_file, K = 5, F = 700)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQdtcvi2sCT0",
        "outputId": "267ee398-36fe-44f6-86e1-614599ccc6cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading dataset...\n",
            "Running Apriori for K=5, F=400\n",
            "\n",
            "Time taken: 1.03 seconds\n",
            "\n",
            "Total Frequent K-itemsets Found: 5\n",
            "Frequent K-itemsets:\n",
            "Itemset: ('bush', 'general', 'kerry', 'poll', 'polls'), Count: 437\n",
            "Itemset: ('bush', 'democratic', 'democrats', 'general', 'republicans'), Count: 409\n",
            "Itemset: ('bush', 'democratic', 'democrats', 'house', 'republicans'), Count: 403\n",
            "Itemset: ('democratic', 'kerry', 'poll', 'polls', 'primary'), Count: 400\n",
            "Itemset: ('bush', 'democratic', 'general', 'kerry', 'republicans'), Count: 400\n"
          ]
        }
      ],
      "source": [
        "main(vocab_file, docword_file, K = 5, F = 400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that Code 2 is taking lesser time than Code 1. Naotable difference :\n",
        "\n",
        "   - Code 1 uses **recursive brute-force generation** of $ K $-itemsets, which is computationally expensive.\n",
        "   - Code 2 uses the **Apriori algorithm**, which prunes infrequent itemsets early, significantly reducing the number of candidates.\n",
        "   - Code 2's iterative and pruning-based approach ensures faster execution times.\n",
        "\n",
        "In summary, **Code 2 is better** because it is optimized for efficiency and performance, making it suitable for mining frequent itemsets in large datasets. Hence, we proceed with Code 2."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
