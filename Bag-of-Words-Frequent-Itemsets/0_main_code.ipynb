{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgXvKnSIuJ3f"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## DMML Assignment 1: Frequent Itemsets\n",
        "\n",
        "**Trishita Patra - MDS202440**  \n",
        "**Boda Surya Venkata Jyothi Sowmya - MDS202413**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyI7avqxlqeX"
      },
      "source": [
        "This file contains the final code used to complete the task of Assignment 1. \n",
        "\n",
        "We aim to compute frequent itemsets for this data. As usual, a K-itemset of words is a collection of words of size K that occur together in the same document. Write a program to find all K-itemsets of words occurring with frequency F, where K and F are parameters to your program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5c0DPNKuIJ5",
        "outputId": "541a387e-7997-4d61-953b-df0261e21bb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Documentation of packeages used:\n",
        "1. The `time` package in Python is part of the standard library and provides various functions to work with time-related operations. In the provided code, the time package is used to measure the execution time of the Apriori algorithm.\n",
        "2. `from collections import defaultdict` is used to import the defaultdict class from Python's built-in collections module. This external library is part of Python's standard library and provides a specialized dictionary subclass that allows us to specify a default value for keys that do not exist in the dictionary. `defaultdict` is advantageous over `.get()`, and eliminates the manual check, making the code cleaner and more efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxXOwgnVlMFG"
      },
      "source": [
        " **Vocabulary File Parsing**:\n",
        "  - The vocabulary file (`vocab.file.txt`) is read using a helper function `read_vocab(file_path)`.\n",
        "  - This function maps word IDs to their corresponding words, creating a dictionary where keys are word IDs and values are the actual words.\n",
        "  - **Input**: Path to the vocabulary file.\n",
        "  - **Output**: A dictionary mapping word IDs to words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "j7dGdZbok5qf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "def read_vocab(file_path):\n",
        "    vocab = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for i, line in enumerate(f, start=1):\n",
        "            vocab[i] = line.strip()\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y1CRE7GmGBG"
      },
      "source": [
        "**Document-Word File Parsing**:\n",
        "  - The document-word file (`docword.file.txt`) is processed using the helper function `read_docword(file_path)`.\n",
        "  - For each nonzero entry, it records the document ID, word ID, and count. These occurrences are stored in a `defaultdict(set)` data structure, where each word ID maps to a set of document IDs containing that word.\n",
        "  - **Input**: Path to the document-word file.\n",
        "  - **Output**: A dictionary (`word_docs`) where keys are word IDs and values are sets of document IDs containing those words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "V8GJC7IemAc6"
      },
      "outputs": [],
      "source": [
        "def read_docword(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        D = int(f.readline().strip())  # Number of documents\n",
        "        W = int(f.readline().strip())  # Number of words\n",
        "        NNZ = int(f.readline().strip())  # Nonzero entries\n",
        "\n",
        "        word_docs = defaultdict(set)  # wordID -> set of documents containing it\n",
        "        for _ in range(NNZ):\n",
        "            doc_id, word_id, count = map(int, f.readline().strip().split())\n",
        "            word_docs[word_id].add(doc_id)\n",
        "\n",
        "    return word_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeErdeXYnDAv"
      },
      "source": [
        "### Apriori Algorithm Implementation *`apriori(word_docs, K, F)`*:\n",
        "- **Purpose**: Identifies frequent itemsets efficiently using the Apriori algorithm.\n",
        "- **Input**:\n",
        "  - `word_docs`: Dictionary mapping word IDs to sets of document IDs.\n",
        "  - `K`: Size of frequent itemsets.\n",
        "  - `F`: Minimum support threshold (minimum number of documents an itemset must appear in).\n",
        "- **Output**: A sorted list of frequent itemsets with their support counts.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Steps and Data Structures:\n",
        "\n",
        "1. **Frequent 1-Itemsets**:\n",
        "   - **Step**: Identify words with document sets â‰¥ `F`.\n",
        "   - **Data Structure**: Dictionary `{(word,): docs}` where keys are single-word tuples, and values are sets of document IDs.\n",
        "   - **Purpose**: Filters out infrequent words early, reducing the search space.\n",
        "\n",
        "2. **Candidate Generation**:\n",
        "   - **Step**: Generate candidate itemsets of size `k` by merging frequent `(k-1)`-itemsets that share the first `(k-2)` elements.\n",
        "   - **Data Structure**: Set for candidates ensures uniqueness.\n",
        "   - **Purpose**: Efficient pruning reduces unnecessary computations by merging only compatible itemsets.\n",
        "\n",
        "3. **Support Counting**:\n",
        "   - **Step**: Compute support for each candidate by intersecting document sets of its constituent words using `set.intersection()`.\n",
        "   - **Data Structure**: Retain frequent itemsets as a dictionary `{itemset: docs}`.\n",
        "\n",
        "4. **Early Termination**:\n",
        "   - **Step**: If no new frequent itemsets are found at any iteration, terminate early.\n",
        "   - **Benefit**: Saves computation time by avoiding unnecessary iterations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-iNJmKnCwrRi"
      },
      "outputs": [],
      "source": [
        "def apriori(word_docs, K, F):\n",
        "\n",
        "    # Step 1: Find frequent 1-itemsets\n",
        "    freq_itemsets = { (word,): docs for word, docs in word_docs.items() if len(docs) >= F }\n",
        "\n",
        "    # Step 2: Generate k-itemsets iteratively\n",
        "    for k in range(2, K + 1):\n",
        "        candidates = set()\n",
        "        freq_keys = list(freq_itemsets.keys())  # List of current frequent itemsets\n",
        "\n",
        "        # Generate candidate itemsets of size k using (k-1)-itemsets\n",
        "        for i in range(len(freq_keys)):\n",
        "            for j in range(i + 1, len(freq_keys)):\n",
        "                a, b = freq_keys[i], freq_keys[j]\n",
        "\n",
        "                # Merge only if first (k-2) elements are same (Efficient pruning)\n",
        "                if a[:-1] == b[:-1]:\n",
        "                    new_itemset = tuple(sorted(set(a) | set(b)))  # Union\n",
        "                    if len(new_itemset) == k:\n",
        "                        candidates.add(new_itemset)\n",
        "\n",
        "        # Count support for candidate itemsets\n",
        "        new_freq_itemsets = {}\n",
        "        for c in candidates:\n",
        "            intersect_docs = set.intersection(*(word_docs[word] for word in c))\n",
        "            if len(intersect_docs) >= F:\n",
        "                new_freq_itemsets[c] = intersect_docs\n",
        "\n",
        "        # If no new frequent itemsets, break early\n",
        "        if not new_freq_itemsets:\n",
        "            return []\n",
        "\n",
        "        freq_itemsets = new_freq_itemsets\n",
        "\n",
        "    # Convert sets to counts for readability\n",
        "    return sorted([(itemset, len(docs)) for itemset, docs in freq_itemsets.items()],\n",
        "                  key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE38A3yhqtkv"
      },
      "source": [
        "`main(vocab_file, docword_file, K, F)`\n",
        "\n",
        "  - Implements the apriori algorithm using aforementioned helper functions, and computes the time taken to run the algorithm for given `(K,F)` pairs, and presents results in a user-friendly format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJggJuGznVCK"
      },
      "outputs": [],
      "source": [
        "def main(vocab_file, docword_file, K, F):\n",
        "    print(\"Reading dataset...\")\n",
        "    word_docs = read_docword(docword_file)\n",
        "    vocab = read_vocab(vocab_file)\n",
        "\n",
        "    print(f\"Running Apriori for K={K}, F={F}\")\n",
        "    start_time = time.time()\n",
        "    frequent_itemsets = apriori(word_docs, K, F)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nTime taken: {elapsed:.2f} seconds\")\n",
        "\n",
        "    if not frequent_itemsets:\n",
        "        print(\"\\nNo itemsets found.\")\n",
        "    else:\n",
        "        print(f\"\\nTotal Frequent K-itemsets Found: {len(frequent_itemsets)}\")\n",
        "        print(\"Frequent K-itemsets:\")\n",
        "        for itemset, count in frequent_itemsets:\n",
        "            print(f\"Itemset: {tuple(vocab[word] for word in itemset)}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuoPDPCqkSI2"
      },
      "source": [
        "Example with Enron dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xJGliG1lu0d7"
      },
      "outputs": [],
      "source": [
        "vocab_file = \"/content/drive/My Drive/DMML_Assignment_1/vocab.enron.txt\"\n",
        "docword_file = \"/content/drive/My Drive/DMML_Assignment_1/docword.enron.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my375vZcxts0",
        "outputId": "13949832-fa34-4e41-a118-f62a19d9f5af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading dataset...\n",
            "Running Apriori for K=3, F=1700\n",
            "\n",
            "Time taken: 25.14 seconds\n",
            "\n",
            "Total Frequent K-itemsets Found: 1\n",
            "Frequent K-itemsets:\n",
            "Itemset: ('energy', 'market', 'power'), Count: 1764\n"
          ]
        }
      ],
      "source": [
        "main(vocab_file, docword_file, K = 3, F = 1700)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yki9UcsYxwRr",
        "outputId": "65c4bfd8-a91b-4e9e-c21e-c8c4eb5d788f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading dataset...\n",
            "Running Apriori for K=3, F=1300\n",
            "\n",
            "Time taken: 51.52 seconds\n",
            "\n",
            "Total Frequent K-itemsets Found: 25\n",
            "Frequent K-itemsets:\n",
            "Itemset: ('energy', 'market', 'power'), Count: 1764\n",
            "Itemset: ('california', 'energy', 'power'), Count: 1667\n",
            "Itemset: ('california', 'market', 'power'), Count: 1602\n",
            "Itemset: ('market', 'power', 'price'), Count: 1481\n",
            "Itemset: ('california', 'energy', 'market'), Count: 1473\n",
            "Itemset: ('energy', 'market', 'price'), Count: 1439\n",
            "Itemset: ('business', 'company', 'market'), Count: 1412\n",
            "Itemset: ('cost', 'energy', 'power'), Count: 1403\n",
            "Itemset: ('energy', 'power', 'price'), Count: 1401\n",
            "Itemset: ('california', 'cost', 'power'), Count: 1398\n",
            "Itemset: ('cost', 'market', 'power'), Count: 1397\n",
            "Itemset: ('electricity', 'energy', 'power'), Count: 1386\n",
            "Itemset: ('market', 'price', 'prices'), Count: 1381\n",
            "Itemset: ('california', 'electricity', 'power'), Count: 1378\n",
            "Itemset: ('market', 'power', 'prices'), Count: 1344\n",
            "Itemset: ('california', 'market', 'price'), Count: 1342\n",
            "Itemset: ('cost', 'energy', 'market'), Count: 1323\n",
            "Itemset: ('cost', 'market', 'price'), Count: 1321\n",
            "Itemset: ('california', 'cost', 'market'), Count: 1320\n",
            "Itemset: ('california', 'electricity', 'energy'), Count: 1318\n",
            "Itemset: ('company', 'energy', 'market'), Count: 1317\n",
            "Itemset: ('companies', 'company', 'market'), Count: 1316\n",
            "Itemset: ('business', 'energy', 'market'), Count: 1313\n",
            "Itemset: ('california', 'power', 'price'), Count: 1312\n",
            "Itemset: ('energy', 'market', 'prices'), Count: 1305\n"
          ]
        }
      ],
      "source": [
        "main(vocab_file, docword_file, K = 3, F = 1300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhEg870yx5Pv",
        "outputId": "6c5e6efa-0191-4e3a-e611-6b583bb458f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading dataset...\n",
            "Running Apriori for K=5, F=1300\n",
            "\n",
            "Time taken: 53.76 seconds\n",
            "\n",
            "No itemsets found.\n"
          ]
        }
      ],
      "source": [
        "main(vocab_file, docword_file, K = 5, F = 1300)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
